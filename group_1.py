# -*- coding: utf-8 -*-
"""group_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16xtL1u5vney-8OEgCQs2Ytg4aAsAGKIW
"""

!pip install transformers datasets pandas numpy scikit-learn matplotlib seaborn

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import pandas as pd

zip_path = '/content/drive/MyDrive/Group 1/pan25-generative-ai-detection-task1-train.zip'
with zipfile.ZipFile(zip_path) as z:
    with z.open('train.jsonl') as f:
        df = pd.read_json(f, lines=True)

print(df.head())

print(df.info())

print(df['model'].value_counts())

print(df['label'].value_counts())

import matplotlib.pyplot as plt
df['label'].value_counts().plot(kind='bar')
plt.show()

print(df['genre'].value_counts())
import matplotlib.pyplot as plt
df['genre'].value_counts().plot(kind='bar')
plt.show()

human__df = df[df['label'] == 0]
human_genre_counts = human__df['genre'].value_counts()
print("Human generated texts by genre:\n" ,human_genre_counts)

import seaborn as sns

df['text_length'] = df['text'].apply(lambda x: len(x.split()))

print(df['text_length'].describe())

sns.histplot(df['text_length'], bins=30, kde=True)
plt.title("Text Length Distribution (in words)")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

from wordcloud import WordCloud

human_text = " ".join(df[df['label'] == 0]['text'].tolist())
ai_text = " ".join(df[df['label'] == 1]['text'].tolist())

wc_human = WordCloud(width=800, height=400, background_color='white').generate(human_text)
wc_ai = WordCloud(width=800, height=400, background_color='black').generate(ai_text)

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(wc_human, interpolation='bilinear')
plt.title("Human Text Word Cloud")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(wc_ai, interpolation='bilinear')
plt.title("AI Text Word Cloud")
plt.axis("off")

plt.show()

df['capital_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x)+1))
df['punctuation_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c in '.,;!?') / (len(x)+1))

print(df.groupby('label')[['capital_ratio', 'punctuation_ratio']].mean())

print("\nHuman-written:")
print(df[df['label'] == 0]['text'].sample(2).values)

print("\nAI-generated:")
print(df[df['label'] == 1]['text'].sample(2).values)

!pip install textstat
import textstat

df['readability'] = df['text'].apply(textstat.flesch_reading_ease)
df.groupby('label')['readability'].describe()

hedges = ["likely", "possibly", "probably", "might", "perhaps", "may", "could"]
df['hedge_count'] = df['text'].apply(lambda x: sum(1 for h in hedges if h in x.lower()))
df.groupby('label')['hedge_count'].mean()

df['starts_with'] = df['text'].apply(lambda x: x.split()[0].lower() if x.split() else 'none')
print(df.groupby(['label', 'starts_with']).size().sort_values(ascending=False).head(10))

df['word_count'] = df['text'].apply(lambda x: len(x.split()))
print(df.groupby('label')['word_count'].describe())

from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer

human_texts = df[df['label'] == 0]['text']
ai_texts = df[df['label'] == 1]['text']

vectorizer = CountVectorizer(stop_words='english', max_features=1000)
human_bow = vectorizer.fit_transform(human_texts).toarray().sum(axis=0)
ai_bow = vectorizer.fit_transform(ai_texts).toarray().sum(axis=0)

human_freq = dict(zip(vectorizer.get_feature_names_out(), human_bow))
ai_freq = dict(zip(vectorizer.get_feature_names_out(), ai_bow))

import pandas as pd
freq_df = pd.DataFrame({
    'word': vectorizer.get_feature_names_out(),
    'human': human_bow,
    'ai': ai_bow
}).sort_values(by='human', ascending=False)

print(freq_df.head(10))

import nltk

nltk.download('punkt')
nltk.download('punkt_tab')

df['sent_count'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))
df['avg_words_per_sent'] = df['word_count'] / (df['sent_count'] + 1)

df.groupby('label')[['sent_count', 'avg_words_per_sent']].mean()

def type_token_ratio(text):
    words = text.split()
    return len(set(words)) / (len(words) + 1)

df['ttr'] = df['text'].apply(type_token_ratio)
df.groupby('label')['ttr'].describe()

from sklearn.model_selection import train_test_split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)

from transformers import AutoTokenizer

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)

import torch

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __len__(self):
        return len(self.labels)
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_dataset = TextDataset(train_encodings, train_labels)
val_dataset = TextDataset(val_encodings, val_labels)

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir='./logs',
    eval_strategy="epoch",
    save_strategy="epoch",
    report_to="none"
    )

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)
trainer.train()
model.save_pretrained("my_finetuned_bert")
tokenizer.save_pretrained("my_finetuned_bert")

!zip -r my_finetuned_bert.zip my_finetuned_bert
from google.colab import files
files.download("my_finetuned_bert.zip")

import numpy as np
import torch
from transformers import AutoTokenizer
import pandas as pd
import zipfile


zip_path_val = '/content/drive/MyDrive/Group 1/pan25-generative-ai-detection-task1-train.zip'
with zipfile.ZipFile(zip_path_val) as z:
    with z.open('val.jsonl') as f:
        df_val = pd.read_json(f, lines=True)

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __len__(self):
        return len(self.labels)
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

model_name = "bert-base-uncased"
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
except NameError:
    model_name = "my_finetuned_bert"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

val_texts = df_val['text'].tolist()
val_labels = df_val['label'].tolist()

val_encodings_predict = tokenizer(val_texts, truncation=True, padding=True)

val_dataset_predict = TextDataset(val_encodings_predict, val_labels)

predictions = trainer.predict(val_dataset_predict)

logits = predictions.predictions

y_pred = np.argmax(logits, axis=1)
y_true = predictions.label_ids

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

print("Classification Report:\n", classification_report(y_true, y_pred, digits=4))

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix

accuracy = accuracy_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
roc_auc = roc_auc_score(y_true, logits[:, 1])

print("Accuracy:", round(accuracy, 4))
print("F1 Score:", round(f1, 4))
print("ROC-AUC:", round(roc_auc, 4))

def predict_text_class(text):
    import torch
    # Ensure model and tokenizer are available and loaded
    if 'model' not in globals() or 'tokenizer' not in globals():
        print("Error: Model or tokenizer not loaded. Please run the training and saving cells first.")
        return None, None # Return None to indicate failure

    model.eval()

    device = next(model.parameters()).device

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    inputs = {key: val.to(device) for key, val in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()
        probability = torch.softmax(logits, dim=1)[0][predicted_class].item()

    label = "Human-written" if predicted_class == 0 else "AI-generated"
    return label, probability # Return the label and probability

print("Enter text to classify (or type 'quit' to exit):")

while True:
    user_input = input("üìù Enter text: ")
    if user_input.lower() == 'quit':
        print("Exiting classifier.")
        break
    elif not user_input.strip(): # Handle empty input
        print("Please enter some text.")
        continue

    label, probability = predict_text_class(user_input)

    if label is not None: # Check if prediction was successful
        print(f"üîç Prediction: {label} (Label: {probability:.4f})")
    print("-" * 30) # Separator for next input